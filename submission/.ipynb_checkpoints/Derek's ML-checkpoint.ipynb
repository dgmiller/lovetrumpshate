{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learnin'\n",
    "\n",
    "This notebook shows the score of several algorithms we have learned about in class. First, you will see the import statements and loading the data into a dataframe. Neutral tweets are removed from the outcome classification. The feature matrix X is defined and the data are ready to train the models. The models we chose to use are Naive Bayes, Gradient Boosting, XGBoost, Logistic Regression, Support Vector Machines, and Discriminant Analysis. After the code and test results are displayed, a discussion about the methods and algorithms will follow. In that discussion, we will review the model assumptions, strengths, and pitfalls. We will also address which algorithms we did not use and why, followed by a brief summary of how these results inform our research question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derek/anaconda2/lib/python2.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'utils' from 'utils.pyc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import statements\n",
    "import utils as ut\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "reload(ut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tOptions\n",
      "\n",
      "            1: trump from lab computer\n",
      "\n",
      "            2: trump from linux mint\n",
      "\n",
      "            3: clean trump from lab computer\n",
      "\n",
      "            4: clean trump from linux mint\n",
      "\n",
      "\n",
      "Enter number >> 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>usr_fol</th>\n",
       "      <th>usr_n_stat</th>\n",
       "      <th>usr_fri</th>\n",
       "      <th>n_weblinks</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>n_hashtags</th>\n",
       "      <th>neu</th>\n",
       "      <th>comp</th>\n",
       "      <th>pos-neg</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-11-09 01:46:29.622</th>\n",
       "      <td>1478681189622</td>\n",
       "      <td>1587.0</td>\n",
       "      <td>17840.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.795</td>\n",
       "      <td>-0.5574</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-09 01:46:29.638</th>\n",
       "      <td>1478681189638</td>\n",
       "      <td>686.0</td>\n",
       "      <td>1897.0</td>\n",
       "      <td>1805.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.663</td>\n",
       "      <td>-0.8235</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-09 01:46:30.684</th>\n",
       "      <td>1478681190684</td>\n",
       "      <td>232.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.8402</td>\n",
       "      <td>0.348</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    ts  usr_fol  usr_n_stat  usr_fri  \\\n",
       "ts                                                                     \n",
       "2016-11-09 01:46:29.622  1478681189622   1587.0     17840.0    253.0   \n",
       "2016-11-09 01:46:29.638  1478681189638    686.0      1897.0   1805.0   \n",
       "2016-11-09 01:46:30.684  1478681190684    232.0        51.0    188.0   \n",
       "\n",
       "                         n_weblinks  n_mentions  n_hashtags    neu    comp  \\\n",
       "ts                                                                           \n",
       "2016-11-09 01:46:29.622           0           1           0  0.795 -0.5574   \n",
       "2016-11-09 01:46:29.638           0           0           0  0.663 -0.8235   \n",
       "2016-11-09 01:46:30.684           0           0           1  0.652  0.8402   \n",
       "\n",
       "                         pos-neg  outcome  \n",
       "ts                                         \n",
       "2016-11-09 01:46:29.622   -0.205     -1.0  \n",
       "2016-11-09 01:46:29.638   -0.337     -1.0  \n",
       "2016-11-09 01:46:30.684    0.348      1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#c,df,T = ut.make_train_test()\n",
    "fname = ut.get_file()\n",
    "T = pd.read_csv(fname)\n",
    "T.index = pd.to_datetime(T['ts'],unit='ms') - pd.DateOffset(hours=7)\n",
    "T['outcome'] = np.around(T['comp'].as_matrix())\n",
    "T.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of tweets with neutral sentiment: 0.451176830223\n",
      "Proportion of tweets with negative sentiment: 0.247292360129\n",
      "Proportion of tweets with positive sentiment: 0.301530809649\n"
     ]
    }
   ],
   "source": [
    "print \"Proportion of tweets with neutral sentiment:\",sum(T.outcome == 0)*1./len(T)\n",
    "print \"Proportion of tweets with negative sentiment:\",sum(T.outcome == -1)*1./len(T)\n",
    "print \"Proportion of tweets with positive sentiment:\",sum(T.outcome == 1)*1./len(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338579\n",
      "185820\n",
      "Index([u'ts', u'usr_fol', u'usr_n_stat', u'usr_fri', u'n_weblinks',\n",
      "       u'n_mentions', u'n_hashtags', u'neu'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print len(T)\n",
    "T = T[T['outcome'] != 0]\n",
    "print len(T)\n",
    "X = T.drop(['outcome','comp','pos-neg'],axis=1)\n",
    "print X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Ha! Typical. So naive. Noob!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61184274284729778"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(X[:\"2016-11-07\"],T[:\"2016-11-07\"]['outcome'])\n",
    "nb.score(X[\"2016-11-08\":],T[\"2016-11-08\":]['outcome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Regression Trees\n",
    "GBRT = Great Britain ReTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61834666781526271"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(X[:\"2016-11-07\"],T[:\"2016-11-07\"]['outcome'])\n",
    "gbc.score(X[\"2016-11-08\":],T[\"2016-11-08\":]['outcome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "Great Britain's more attractive cousin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61574079059299858"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc = xgb.XGBClassifier()\n",
    "xgbc.fit(X[:\"2016-11-07\"],T[:\"2016-11-07\"]['outcome'])\n",
    "xgbc.score(X[\"2016-11-08\":],T[\"2016-11-08\":]['outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61863740618303598"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc = xgb.XGBClassifier(gamma=2,reg_lambda=.5)\n",
    "xgbc.fit(X[:\"2016-11-07\"],T[:\"2016-11-07\"]['outcome'])\n",
    "xgbc.score(X[\"2016-11-08\":],T[\"2016-11-08\":]['outcome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "This one goes to eleven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38295627079586936"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#c,df,T = ut.make_train_test()\n",
    "\n",
    "## Vanilla implementation\n",
    "lrc = LogisticRegression()\n",
    "lrc.fit(X[:\"2016-11-07\"],T[:\"2016-11-07\"]['outcome'])\n",
    "lrc.score(X[\"2016-11-08\":],T[\"2016-11-08\":]['outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38295627079586936"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Chocolate Regression\n",
    "lrc = LogisticRegression(C=.1,solver='lbfgs',multi_class='multinomial')\n",
    "lrc.fit(X[:\"2016-11-07\"],T[:\"2016-11-07\"]['outcome'])\n",
    "lrc.score(X[\"2016-11-08\":],T[\"2016-11-08\":]['outcome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "Rise of the Machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38295627079586936"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC,SVC\n",
    "lsvc = LinearSVC()\n",
    "lsvc.fit(X[:\"2016-11-07\"],T[:\"2016-11-07\"]['outcome'])\n",
    "lsvc.score(X[\"2016-11-08\":],T[\"2016-11-08\":]['outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61704372920413064"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(kernel='poly')\n",
    "svc.fit(X[:\"2016-11-07\"],T[:\"2016-11-07\"]['outcome'])\n",
    "svc.score(X[\"2016-11-08\":],T[\"2016-11-08\":]['outcome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminant Analysis\n",
    "A nice way of saying 'racial profiling'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5827904422453617"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X[:\"2016-11-07\"],T[:\"2016-11-07\"]['outcome'])\n",
    "qda.score(X[\"2016-11-08\":],T[\"2016-11-08\":]['outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61704372920413064"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qda = QuadraticDiscriminantAnalysis(reg_param=1)\n",
    "qda.fit(X[:\"2016-11-07\"],T[:\"2016-11-07\"]['outcome'])\n",
    "qda.score(X[\"2016-11-08\":],T[\"2016-11-08\":]['outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61148739595335266"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qda = QuadraticDiscriminantAnalysis(reg_param=.9)\n",
    "qda.fit(X[:\"2016-11-07\"],T[:\"2016-11-07\"]['outcome'])\n",
    "qda.score(X[\"2016-11-08\":],T[\"2016-11-08\":]['outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61035674674534546"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qda = QuadraticDiscriminantAnalysis(reg_param=.7)\n",
    "qda.fit(X[:\"2016-11-07\"],T[:\"2016-11-07\"]['outcome'])\n",
    "qda.score(X[\"2016-11-08\":],T[\"2016-11-08\":]['outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55770079791529825"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X[:\"2016-11-07\"],T[:\"2016-11-07\"]['outcome'])\n",
    "lda.score(X[\"2016-11-08\":],T[\"2016-11-08\":]['outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58794835625141328"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis(solver='lsqr',shrinkage='auto')\n",
    "lda.fit(X[:\"2016-11-07\"],T[:\"2016-11-07\"]['outcome'])\n",
    "lda.score(X[\"2016-11-08\":],T[\"2016-11-08\":]['outcome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the algorithms we learned from class were either not well suited for our problem or were more complicated to implement and not worth implementing at this stage. Here are those algorithms and a brief explanation.\n",
    "\n",
    "+ Nearest Neighbors: the concept of distance does not suit our problem well. It is still possible to use this as a classifier, but the model is not intuitive for a time-sensitive sentiment classification.\n",
    "+ Linear Regression: while we could try to predict the sentiment score calculated from the nltk package, we decided not to do this and so linear regression is not a good binary classifier.\n",
    "+ Ridge Regression: this is still regression and not well suited to our problem for the same reasons as above.\n",
    "+ Mixture Models with Latent Variables: this is good for topic extraction and may be a good way to improve our results through feature engineering. However, modeling the time-sensitive tweet data as a network is also not intuitive, though some aspects of Twitter are certainly networks. Ultimately, this is not ideal for sentiment classification.\n",
    "+ Decision Trees: these work well but only when multiple trees are trained.\n",
    "+ Random Forests: better than a decision tree but inferior to gradient boosting.\n",
    "+ Kalman Filter: sentiment classification does not have a clear state space model that it relies on, though this may be useful in the future.\n",
    "+ ARMA: similar to the Kalman Filter above. Need more information to set up the model.\n",
    "+ Neural Networks: this will be very good for classification based on the text. However, the model is very complicated and simpler algorithms are likely to perform well enough---at least for a benchmark.\n",
    "\n",
    "In order, the vanilla (out-of-the-box) implementations of our algorithms gave us these results for their rank according to test score:\n",
    "\n",
    "+ Gradient Boosting (.6183)\n",
    "+ Polynomial SVM (.6170)\n",
    "+ XGBoost (.6157)\n",
    "+ Naive Bayes (.6118)\n",
    "+ Quad Discriminant Analysis (.5828)\n",
    "+ Linear Discriminant Analysis (.5577)\n",
    "+ Logistic Regression (.3830)\n",
    "+ Linear SVM (.3830)\n",
    "\n",
    "Many of these did not improve with modification or regularization. In particular, Linear SVM and Logistic Regression were very poor performers and did not improve when the parameters were changed. Interestingly, Quad Discriminant Analysis performed poorly when reg_param > 1, getting the same score as logistic regression and linear SVM. Most of the other algorithms improved when parameters were changed or tweaked. After some experimentation, the following algorithms performed best based on the highest score achieved.\n",
    "\n",
    "+ XGBoost (.6186)\n",
    "+ Gradient Boosting (.6183)\n",
    "+ Polynomial SVM (.6170)\n",
    "+ Quad Discriminant Analysis (.6170)\n",
    "+ Naive Bayes (.6118)\n",
    "+ Linear Discriminant Analysis (.5879)\n",
    "+ Logistic Regression (.3830)\n",
    "+ Linear SVM (.3830)\n",
    "\n",
    "It's interesting that Gradient Boosting performed better out of the box than XGBoost but XGBoost improved a fair amount with tuned regularization. The largest gain in improvement came from the SVM family, where polynomial SVM was much better than Linear SVM. There does appear to be something funky going on since the scores between these two models sum to one. Quad Discriminant Analysis also gained quite a bit---from .5828 to .6170 making it go from 5th tied with Polynomial SVM for 3rd best.\n",
    "\n",
    "Of course, these scores don't tell us we have a good model. It remains to be seen how the models interpret new tweets and to see if those tweets are truly positive or negative toward Trump. However, it does appear that tree based models are clearly advantageous for our problem (at least, without using other features from the text data). Polynomial SVM and Quad Discriminant Analysis also warrant more investigation and experimentation. Since Linear SVM and Logistic Regression were such poor performers, they are likely not the best algorithms to use. As noted before, there is something suspicious about their scores. It would be unwise to reject these algorithms outright. Manipuplation of the data or using some other parameters might improve these models. This seems unlikely though.\n",
    "\n",
    "The key to using tree-based methods for our problem is knowing how to adjust the regularization, since trees are well-known to overfit the data. Next steps include cross validation with tree-based models and Poly SVM and QDA. Feature engineering on the text data may be useful as well. However, textual data may best be modeled by something more complex, like a Neural Network. It is unfortunate that the individual models don't get much better than about 62% accurate. An ensemble model might improve the score drastically as each model may capture different information about Trump sentiment. What this tells us is that despite trying to make the outcome variable either very pro or very anti-Trump, the problem of sentiment classification is very hard---especially when relying on models and data built into the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
